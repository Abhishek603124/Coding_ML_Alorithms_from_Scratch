{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGlZR7sCfcfIuWlqb3Srme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek603124/Coding_ML_Alorithms_from_Scratch/blob/main/Coding_ML_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud1PCp4YcWdI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "Aw1g22vZcbSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_Regression:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.m = None\n",
        "    self.b = None\n",
        "\n",
        "\n",
        "  def fit(self,x_train,y_train):\n",
        "    x_train_mean = np.mean(x_train)\n",
        "    y_train_mean = np.mean(y_train)\n",
        "    num = 0\n",
        "    den = 0\n",
        "    num = np.sum((y_train-y_train_mean)*(x_train-x_train_mean))\n",
        "    den = np.sum((x_train-x_train_mean)**2)\n",
        "    if den == 0:\n",
        "      print(\"Can't compute slope as the denominator is 0\")\n",
        "      self.m = 0\n",
        "      self.b = y_train_mean\n",
        "    else:\n",
        "      self.m = num / den\n",
        "      self.b = y_train_mean - self.m * x_train_mean\n",
        "  def predict(self,x_test):\n",
        "     return self.m*x_test + self.b"
      ],
      "metadata": {
        "id": "JlWBDb3Bcana"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "qNQPHyPsctj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sigmoid(self,x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "class LOR():\n",
        "  def __init__(self,lr,iters):\n",
        "    self.lr=0.01\n",
        "    self.iters = 1000\n",
        "  def fit(self,x_train,y_train):\n",
        "    n_samples,n_features = x.shape[0]\n",
        "    self.weights = np.zeros(n_features)\n",
        "    self.bias = 0\n",
        "\n",
        "    for i in range(self.iters):\n",
        "      self.linear_pred = np.dot(x_train,self.weights) + self.bias\n",
        "      predictions = sigmoid(linear_pred)\n",
        "\n",
        "    # gradients\n",
        "      dw = (1/n_samples)*np.dot(x_train.T,(predictions-y_train))\n",
        "      db = (1/n_samples)*np.sum(predictions-y_train)\n",
        "    # updates\n",
        "      self.weights-=self.lr*dw\n",
        "      self.bias-=self.lr*db\n",
        "\n",
        "  def predict(self,x_test):\n",
        "    self.linear_pred = np.dot(x_test,self.weights) + self.bias\n",
        "    predictions = sigmoid(linear_pred)\n",
        "    class_predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
        "    return class_predictions"
      ],
      "metadata": {
        "id": "k9AVBah3cj1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbour"
      ],
      "metadata": {
        "id": "ShHC9MqKc0lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def eucledian_distance(x, x_train):\n",
        "    return np.sqrt(np.sum((x - x_train) ** 2))\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return predictions\n",
        "\n",
        "    def _predict(self, x):\n",
        "        distances = [eucledian_distance(x, x_train) for x_train in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        most_common = Counter(k_nearest_labels).most_common()\n",
        "        return most_common[0][0]\n"
      ],
      "metadata": {
        "id": "jBhABwDdcq-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree\n",
        "Little tricky"
      ],
      "metadata": {
        "id": "1hYuc9xdc8wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.max_depth=max_depth\n",
        "        self.n_features=n_features\n",
        "        self.root=None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # check the stopping criteria\n",
        "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "\n",
        "        # find the best split\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # create child nodes\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, X_column, thr)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        # parent entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # create children\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # calculate the weighted avg. entropy of children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)"
      ],
      "metadata": {
        "id": "i3EHJAMUc3pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "bQ0NyqGjdQ7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Naive_Bayes:\n",
        "\n",
        "  def fit(self,X,y):\n",
        "    n_samples,n_features = X.shape\n",
        "    self._classes = np.unique(y)\n",
        "    n_classes = len(self._classes)\n",
        "\n",
        "    # calculating the mean, variance, and the prior for each class\n",
        "    self._mean = np.zeros((n_classes,n_features),dtype = np.float64)\n",
        "    self._var = np.zeros((n_classes,n_features),dtype = np.float64)\n",
        "    self._priors = np.zeros((n_classes),dtype = np.float64)\n",
        "\n",
        "    for idx , c in enumerate(self._classes):\n",
        "      X_c = X[y==c]\n",
        "      self._mean[idx, :] = X_c.mean(axis=0)\n",
        "      self._var[idx,:] = X_c.var(axis=0)\n",
        "      self._priors[idx] = X_c.shape[0]/float(n_samples)\n",
        "\n",
        "  def predict(self,X):\n",
        "    y_pred = [self._predict(x) for x in X]\n",
        "    return np.array(y_pred)\n",
        "\n",
        "  def _predict(self,x):\n",
        "    posteriors = []\n",
        "\n",
        "    # calculate posterior probability for each class\n",
        "    for idx, c in enumerate(self._classes):\n",
        "      prior = np.log(self._priors[idx])\n",
        "      posterior = np.sum(np.log(self._pdf(idx,x)))\n",
        "      posterior += prior\n",
        "      posteriors.append(posterior)\n",
        "\n",
        "      # return class with higher posterior\n",
        "    return self._classes[np.argmax(posteriors)]\n",
        "\n",
        "  def _pdf(self,class_idx,x):\n",
        "    mean = self._mean[class_idx]\n",
        "    var = self._var[class_idx]\n",
        "    numerator = np.exp(((x - mean) ** 2) / (2 * var))\n",
        "    denominator = np.sqrt(2 * np.pi * var)\n",
        "    return numerator / denominator"
      ],
      "metadata": {
        "id": "34MmY9fTdFY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM(Support Vector Machine)"
      ],
      "metadata": {
        "id": "jbRQ4z15dYmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class SVM:\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        y_ = np.where(y <= 0, -1, 1)\n",
        "\n",
        "        # init weights\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "                if condition:\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "                    self.b -= self.lr * y_[idx]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        approx = np.dot(X, self.w) - self.b\n",
        "        return np.sign(approx)"
      ],
      "metadata": {
        "id": "qqpWEkggdUTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Means Clustering"
      ],
      "metadata": {
        "id": "B2ncJ8GudiQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class KMeans():\n",
        "  def __init__(self,K=5,max_iters=100,plot_steps=False):\n",
        "    self.K=K\n",
        "    self.max_iters=max_iters\n",
        "    self.plot_steps=plot_steps\n",
        "# list of sample indices for each clusters\n",
        "    self.clusters = [[] for _ in range(self.K)]\n",
        "\n",
        "    # centers(mean) vectors for each clusters\n",
        "    self.centroids = []\n",
        "\n",
        "  def predict(self,X):\n",
        "    self.X=X\n",
        "    self.n_samples,self.n_features=X.shape\n",
        "\n",
        "    # initialize the centroids\n",
        "    random_sample_indices = np.random.choice(self.n_samples,self.K, replace = False)\n",
        "    self.centroids = [self.X[idx] for idx in random_sample_indices]\n",
        "\n",
        "    # optimizing the clusters\n",
        "    for _ in range(self.max_iters):\n",
        "      # assign samples to the nearest clusters(create clusters)\n",
        "      self.clusters = self.create_clusters(self.centroids)\n",
        "\n",
        "      # calculate new centroids\n",
        "      centroids_old = self.centroids\n",
        "      self.centroids = self._get_centroids(self.clusters)\n",
        "  def create_clusters(self,centroids):\n",
        "    pass\n",
        "  def _get_centroids(self,clusters):\n",
        "    pass\n",
        "  def _is_converged(self,centroids_old,centroids):\n",
        "    pass"
      ],
      "metadata": {
        "id": "MR2Q5RhidckV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_bWIb9NdnCz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}